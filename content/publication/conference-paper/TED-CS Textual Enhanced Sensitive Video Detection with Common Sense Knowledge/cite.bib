@InProceedings{10.1007/978-3-031-46664-9_46,
author="Yu, Bihui
and Sun, Linzhuang
and Wei, Jingxuan
and Tan, Shuyue
and Zhao, Yiman
and Bu, Liping",
editor="Yang, Xiaochun
and Suhartanto, Heru
and Wang, Guoren
and Wang, Bin
and Jiang, Jing
and Li, Bing
and Zhu, Huaijie
and Cui, Ningning",
title="TED-CS: Textual Enhanced Sensitive Video Detection withÂ Common Sense Knowledge",
booktitle="Advanced Data Mining and Applications",
year="2023",
publisher="Springer Nature Switzerland",
address="Cham",
pages="692--707",
abstract="In the era of short videos, the task of sensitive video detection faces new challenges with the increasing diversity and quantity of videos in the network. Aiming at the problem that existing research methods are constrained by missing video comments and user subjectivity, a novel text enhancement method is proposed for sensitive video detection. Firstly, based on the CLIP pre-training model, image caption are generated for video frames. Then, through the integration of external common sense knowledge, the method extracts deep contextual information from the generated captions, including the underlying intentions and purposes conveyed by the text. Besides, considering the complementarity and redundancy between different sources of information, a multi-source data collaborative encoding mechanism and a multi-modal feature fusion mechanism are designed to achieve semantic feature alignment. Finally, state-of-the-art was achieved on the two public datasets NPDI-800 and Pornography-2k, and a large number of detailed comparison and ablation experiments were performed to verify the effectiveness of the method.",
isbn="978-3-031-46664-9"
}

