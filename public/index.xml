<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>AI For Multimodal Cognitive Intelligence Lab | 多模态认知智能实验室</title>
    <link>https://example.com/</link>
      <atom:link href="https://example.com/index.xml" rel="self" type="application/rss+xml" />
    <description>AI For Multimodal Cognitive Intelligence Lab</description>
    <generator>Hugo Blox Builder (https://hugoblox.com)</generator><language>en-us</language><lastBuildDate>Mon, 24 Oct 2022 00:00:00 +0000</lastBuildDate>
    <image>
      <url>https://example.com/media/logo_hu_81c9b54ff9ed5c81.png</url>
      <title>AI For Multimodal Cognitive Intelligence Lab</title>
      <link>https://example.com/</link>
    </image>
    
    <item>
      <title>Example Event</title>
      <link>https://example.com/event/example/</link>
      <pubDate>Sat, 01 Jun 2030 13:00:00 +0000</pubDate>
      <guid>https://example.com/event/example/</guid>
      <description>&lt;p&gt;Slides can be added in a few ways:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Create&lt;/strong&gt; slides using Wowchemy&amp;rsquo;s &lt;a href=&#34;https://docs.hugoblox.com/managing-content/#create-slides&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;em&gt;Slides&lt;/em&gt;&lt;/a&gt; feature and link using &lt;code&gt;slides&lt;/code&gt; parameter in the front matter of the talk file&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Upload&lt;/strong&gt; an existing slide deck to &lt;code&gt;static/&lt;/code&gt; and link using &lt;code&gt;url_slides&lt;/code&gt; parameter in the front matter of the talk file&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Embed&lt;/strong&gt; your slides (e.g. Google Slides) or presentation video on this page using &lt;a href=&#34;https://docs.hugoblox.com/writing-markdown-latex/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;shortcodes&lt;/a&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Further event details, including page elements such as image galleries, can be added to the body of this page.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>From Words to Structured Visuals: A Benchmark and Framework for Text-to-Diagram Generation and Editing</title>
      <link>https://example.com/publication/from-words-to-structured-copy/</link>
      <pubDate>Tue, 01 Jul 2025 00:00:00 +0000</pubDate>
      <guid>https://example.com/publication/from-words-to-structured-copy/</guid>
      <description></description>
    </item>
    
    <item>
      <title>From Words to Structured Visuals: A Benchmark and Framework for Text-to-Diagram Generation and Editing</title>
      <link>https://example.com/publication/from-words-to-structured/</link>
      <pubDate>Tue, 01 Jul 2025 00:00:00 +0000</pubDate>
      <guid>https://example.com/publication/from-words-to-structured/</guid>
      <description></description>
    </item>
    
    <item>
      <title>EEGTCW: Electroencephalogram-based Chinese Words Decoding</title>
      <link>https://example.com/publication/eegtcw-copy/</link>
      <pubDate>Mon, 23 Jun 2025 00:00:00 +0000</pubDate>
      <guid>https://example.com/publication/eegtcw-copy/</guid>
      <description></description>
    </item>
    
    <item>
      <title>EEGTCW: Electroencephalogram-based Chinese Words Decoding</title>
      <link>https://example.com/publication/preprint/</link>
      <pubDate>Mon, 23 Jun 2025 00:00:00 +0000</pubDate>
      <guid>https://example.com/publication/preprint/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Multifaceted Assessment and Resolution of Hallucinations in Large Visual-Language Models</title>
      <link>https://example.com/publication/multifaceted-assessment-and-copy/</link>
      <pubDate>Mon, 23 Jun 2025 00:00:00 +0000</pubDate>
      <guid>https://example.com/publication/multifaceted-assessment-and-copy/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Multifaceted Assessment and Resolution of Hallucinations in Large Visual-Language Models</title>
      <link>https://example.com/publication/multifaceted-assessment-and/</link>
      <pubDate>Mon, 23 Jun 2025 00:00:00 +0000</pubDate>
      <guid>https://example.com/publication/multifaceted-assessment-and/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Congratulations on CVPR 2025 Publication</title>
      <link>https://example.com/post/congratulations-on-cvpr-2025-publication/</link>
      <pubDate>Thu, 27 Feb 2025 00:00:00 +0000</pubDate>
      <guid>https://example.com/post/congratulations-on-cvpr-2025-publication/</guid>
      <description>&lt;p&gt;中国科学院沈阳计算所多模态认知智能团队“From Words to Structured Visuals: A Benchmark and Framework for Text-to-Diagram Generation and Editing”文章被计算机视觉与模式识别会议(CVPR 2025)接受，CVPR是计算机视觉领域最具权威、最高水平、最具影响力的国际顶级学术会议之一，为CCF A类会议。&lt;/p&gt;
&lt;p&gt;We are excited to celebrate the acceptance of the paper &amp;ldquo;From Words to Structured Visuals: A Benchmark and Framework for Text-to-Diagram Generation and Editing&amp;rdquo; by Jingxuan Wei, Cheng Tan, Qi Chen, Gaowei Wu, Siyuan Li, Zhangyang Gao, Linzhuang Sun, Bihui Yu, and Ruifeng Guo at CVPR 2025!&lt;/p&gt;
&lt;p&gt;This innovative work  presents a pioneering benchmark and framework for text-to-diagram generation and editing. The paper introduces a comprehensive approach to transforming textual descriptions into structured visual diagrams, addressing challenges in automated diagram creation and iterative editing. By establishing a new benchmark, the authors provide a robust evaluation platform for future research in this interdisciplinary domain of computer vision and natural language processing. Their framework leverages advanced techniques to enable precise, context-aware diagram generation, offering significant potential for applications in education, technical documentation, and data visualization.&lt;/p&gt;
&lt;p&gt;This achievement highlights the team&amp;rsquo;s dedication to advancing the integration of language and visual processing, paving the way for more intuitive and automated visualization tools. Congratulations to Jingxuan Wei, Cheng Tan, Qi Chen, Gaowei Wu, Siyuan Li, Zhangyang Gao, Linzhuang Sun, Bihui Yu, and Ruifeng Guo for their outstanding contribution to CVPR 2025!&lt;/p&gt;
&lt;p&gt;Read the full paper here: &lt;a href=&#34;https://arxiv.org/pdf/2411.11916&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://arxiv.org/pdf/2411.11916&lt;/a&gt;&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Contact</title>
      <link>https://example.com/contact/</link>
      <pubDate>Mon, 24 Oct 2022 00:00:00 +0000</pubDate>
      <guid>https://example.com/contact/</guid>
      <description></description>
    </item>
    
    <item>
      <title>People</title>
      <link>https://example.com/people/</link>
      <pubDate>Mon, 24 Oct 2022 00:00:00 +0000</pubDate>
      <guid>https://example.com/people/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Tour</title>
      <link>https://example.com/tour/</link>
      <pubDate>Mon, 24 Oct 2022 00:00:00 +0000</pubDate>
      <guid>https://example.com/tour/</guid>
      <description></description>
    </item>
    
    <item>
      <title></title>
      <link>https://example.com/admin/config.yml</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://example.com/admin/config.yml</guid>
      <description></description>
    </item>
    
  </channel>
</rss>
