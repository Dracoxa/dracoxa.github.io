<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>AI For Multimodal Cognitive Intelligence Lab | 多模态认知智能实验室</title>
    <link>https://example.com/</link>
      <atom:link href="https://example.com/index.xml" rel="self" type="application/rss+xml" />
    <description>AI For Multimodal Cognitive Intelligence Lab</description>
    <generator>Hugo Blox Builder (https://hugoblox.com)</generator><language>en-us</language><lastBuildDate>Mon, 24 Oct 2022 00:00:00 +0000</lastBuildDate>
    <image>
      <url>https://example.com/media/logo_hu_81c9b54ff9ed5c81.png</url>
      <title>AI For Multimodal Cognitive Intelligence Lab</title>
      <link>https://example.com/</link>
    </image>
    
    <item>
      <title>Example Event</title>
      <link>https://example.com/event/example/</link>
      <pubDate>Sat, 01 Jun 2030 13:00:00 +0000</pubDate>
      <guid>https://example.com/event/example/</guid>
      <description>&lt;p&gt;Slides can be added in a few ways:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Create&lt;/strong&gt; slides using Wowchemy&amp;rsquo;s &lt;a href=&#34;https://docs.hugoblox.com/managing-content/#create-slides&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;em&gt;Slides&lt;/em&gt;&lt;/a&gt; feature and link using &lt;code&gt;slides&lt;/code&gt; parameter in the front matter of the talk file&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Upload&lt;/strong&gt; an existing slide deck to &lt;code&gt;static/&lt;/code&gt; and link using &lt;code&gt;url_slides&lt;/code&gt; parameter in the front matter of the talk file&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Embed&lt;/strong&gt; your slides (e.g. Google Slides) or presentation video on this page using &lt;a href=&#34;https://docs.hugoblox.com/writing-markdown-latex/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;shortcodes&lt;/a&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Further event details, including page elements such as image galleries, can be added to the body of this page.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>From Words to Structured Visuals: A Benchmark and Framework for Text-to-Diagram Generation and Editing</title>
      <link>https://example.com/publication/conference-paper/from-words-to-structured/</link>
      <pubDate>Tue, 01 Jul 2025 00:00:00 +0000</pubDate>
      <guid>https://example.com/publication/conference-paper/from-words-to-structured/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Multifaceted Assessment and Resolution of Hallucinations in Large Visual-Language Models</title>
      <link>https://example.com/publication/conference-paper/multifaceted-assessment-and/</link>
      <pubDate>Mon, 23 Jun 2025 00:00:00 +0000</pubDate>
      <guid>https://example.com/publication/conference-paper/multifaceted-assessment-and/</guid>
      <description></description>
    </item>
    
    <item>
      <title>ChartReasoner: Code-Driven Modality Bridging for Long-Chain Reasoning in Chart Question Answering</title>
      <link>https://example.com/publication/preprint/chartmind-a-comprehensive-benchmark-for-complex-real-world-multimodal-chart-question-answering/</link>
      <pubDate>Mon, 14 Apr 2025 15:57:35 +0000</pubDate>
      <guid>https://example.com/publication/preprint/chartmind-a-comprehensive-benchmark-for-complex-real-world-multimodal-chart-question-answering/</guid>
      <description></description>
    </item>
    
    <item>
      <title>ChartReasoner: Code-Driven Modality Bridging for Long-Chain Reasoning in Chart Question Answering</title>
      <link>https://example.com/publication/preprint/chartreasoner-code-driven-modality-bridging-for-long-chain-reasoning-in-chart-question-answering/</link>
      <pubDate>Mon, 14 Apr 2025 15:57:35 +0000</pubDate>
      <guid>https://example.com/publication/preprint/chartreasoner-code-driven-modality-bridging-for-long-chain-reasoning-in-chart-question-answering/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Congratulations on CVPR 2025 Publication</title>
      <link>https://example.com/post/congratulations-on-cvpr-2025-publication/</link>
      <pubDate>Thu, 27 Feb 2025 00:00:00 +0000</pubDate>
      <guid>https://example.com/post/congratulations-on-cvpr-2025-publication/</guid>
      <description>&lt;p&gt;中国科学院沈阳计算所多模态认知智能团队“From Words to Structured Visuals: A Benchmark and Framework for Text-to-Diagram Generation and Editing”文章被计算机视觉与模式识别会议(CVPR 2025)接受，CVPR是计算机视觉领域最具权威、最高水平、最具影响力的国际顶级学术会议之一，为CCF A类会议。&lt;/p&gt;
&lt;p&gt;We are excited to celebrate the acceptance of the paper &amp;ldquo;From Words to Structured Visuals: A Benchmark and Framework for Text-to-Diagram Generation and Editing&amp;rdquo; by Jingxuan Wei, Cheng Tan, Qi Chen, Gaowei Wu, Siyuan Li, Zhangyang Gao, Linzhuang Sun, Bihui Yu, and Ruifeng Guo at CVPR 2025!&lt;/p&gt;
&lt;p&gt;This innovative work  presents a pioneering benchmark and framework for text-to-diagram generation and editing. The paper introduces a comprehensive approach to transforming textual descriptions into structured visual diagrams, addressing challenges in automated diagram creation and iterative editing. By establishing a new benchmark, the authors provide a robust evaluation platform for future research in this interdisciplinary domain of computer vision and natural language processing. Their framework leverages advanced techniques to enable precise, context-aware diagram generation, offering significant potential for applications in education, technical documentation, and data visualization.&lt;/p&gt;
&lt;p&gt;This achievement highlights the team&amp;rsquo;s dedication to advancing the integration of language and visual processing, paving the way for more intuitive and automated visualization tools. Congratulations to Jingxuan Wei, Cheng Tan, Qi Chen, Gaowei Wu, Siyuan Li, Zhangyang Gao, Linzhuang Sun, Bihui Yu, and Ruifeng Guo for their outstanding contribution to CVPR 2025!&lt;/p&gt;
&lt;p&gt;Read the full paper here: &lt;a href=&#34;https://arxiv.org/pdf/2411.11916&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://arxiv.org/pdf/2411.11916&lt;/a&gt;&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Faster and More Efficient Subject Image Generation for Text-to-Image Diffusion Models</title>
      <link>https://example.com/publication/conference-paper/faster-and-more-efficient-subject-image-generation-for-text-to-image-diffusion-models/</link>
      <pubDate>Sun, 06 Oct 2024 15:57:35 +0000</pubDate>
      <guid>https://example.com/publication/conference-paper/faster-and-more-efficient-subject-image-generation-for-text-to-image-diffusion-models/</guid>
      <description></description>
    </item>
    
    <item>
      <title>SAM-Wav2lip&#43;&#43;: Enhancing Behavioral Realism in Synthetic Agents Through Audio-Driven Speech and Action Refinement</title>
      <link>https://example.com/publication/conference-paper/sam-wav2lip&#43;&#43;-enhancing-behavioral-realism-in-synthetic-agents-through-audio-driven-speech-and-action-refinement/</link>
      <pubDate>Sun, 06 Oct 2024 15:57:35 +0000</pubDate>
      <guid>https://example.com/publication/conference-paper/sam-wav2lip&#43;&#43;-enhancing-behavioral-realism-in-synthetic-agents-through-audio-driven-speech-and-action-refinement/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Interpretable and Generalizable Spatiotemporal Predictive Learning with Disentangled Consistency</title>
      <link>https://example.com/publication/conference-paper/interpretable-and-generalizable-spatiotemporal-predictive-learning-with-disentangled-consistency/</link>
      <pubDate>Thu, 22 Aug 2024 15:57:35 +0000</pubDate>
      <guid>https://example.com/publication/conference-paper/interpretable-and-generalizable-spatiotemporal-predictive-learning-with-disentangled-consistency/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Boosting the Power of Small Multimodal Reasoning Models to Match Larger Models with Self-Consistency Training</title>
      <link>https://example.com/publication/conference-paper/boosting-the-power-of-small-multimodal-reasoning-models-to-match-larger-models-with-self-consistency-training/</link>
      <pubDate>Wed, 03 Jul 2024 02:56:47 +0000</pubDate>
      <guid>https://example.com/publication/conference-paper/boosting-the-power-of-small-multimodal-reasoning-models-to-match-larger-models-with-self-consistency-training/</guid>
      <description></description>
    </item>
    
    <item>
      <title>A survey on advancements in image-text multimodal models: From general techniques to biomedical implementations</title>
      <link>https://example.com/publication/conference-paper/a-survey-on-advancements-in-image-text-multimodal-models-from-general-techniques-to-biomedical-implementations/</link>
      <pubDate>Mon, 03 Jun 2024 15:57:35 +0000</pubDate>
      <guid>https://example.com/publication/conference-paper/a-survey-on-advancements-in-image-text-multimodal-models-from-general-techniques-to-biomedical-implementations/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Sentence-Level or Token-Level: A Comprehensive Study on Knowledge Distillation</title>
      <link>https://example.com/publication/conference-paper/sentence-level-or-token-level-a-comprehensive-study-on-knowledge-distillation/</link>
      <pubDate>Tue, 23 Apr 2024 08:29:56 +0000</pubDate>
      <guid>https://example.com/publication/conference-paper/sentence-level-or-token-level-a-comprehensive-study-on-knowledge-distillation/</guid>
      <description></description>
    </item>
    
    <item>
      <title>EM-CNN-BiLSTM: Causality extraction based on Electra and Dual-Stage Attention Mechanism</title>
      <link>https://example.com/publication/conference-paper/em-cnn-bilstm-causality-extraction-based-on-electra-and-dual-stage-attention-mechanism/</link>
      <pubDate>Fri, 08 Dec 2023 15:57:35 +0000</pubDate>
      <guid>https://example.com/publication/conference-paper/em-cnn-bilstm-causality-extraction-based-on-electra-and-dual-stage-attention-mechanism/</guid>
      <description></description>
    </item>
    
    <item>
      <title>TED-CS: Textual Enhanced Sensitive Video Detection with Common Sense Knowledge</title>
      <link>https://example.com/publication/conference-paper/ted-cs-textual-enhanced-sensitive-video-detection-with-common-sense-knowledge/</link>
      <pubDate>Sun, 05 Nov 2023 15:57:35 +0000</pubDate>
      <guid>https://example.com/publication/conference-paper/ted-cs-textual-enhanced-sensitive-video-detection-with-common-sense-knowledge/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Enhancing Human-like Multi-Modal Reasoning: A New Challenging Dataset and Comprehensive Framework</title>
      <link>https://example.com/publication/conference-paper/enhancing-human-like-multi-modal-reasoning-a-new-challenging-dataset-and-comprehensive-framework/</link>
      <pubDate>Tue, 25 Jul 2023 15:57:35 +0000</pubDate>
      <guid>https://example.com/publication/conference-paper/enhancing-human-like-multi-modal-reasoning-a-new-challenging-dataset-and-comprehensive-framework/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Construction of Sensitive Image Datasets Based on Generative Methods</title>
      <link>https://example.com/publication/conference-paper/a-new-multimodal-video-detection-model-and-dataset/</link>
      <pubDate>Fri, 14 Apr 2023 15:57:35 +0000</pubDate>
      <guid>https://example.com/publication/conference-paper/a-new-multimodal-video-detection-model-and-dataset/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Construction of Sensitive Image Datasets Based on Generative Methods</title>
      <link>https://example.com/publication/conference-paper/construction-of-sensitive-image-datasets-based-on-generative-methods/</link>
      <pubDate>Fri, 14 Apr 2023 15:57:35 +0000</pubDate>
      <guid>https://example.com/publication/conference-paper/construction-of-sensitive-image-datasets-based-on-generative-methods/</guid>
      <description></description>
    </item>
    
    <item>
      <title>EEGTCW: Electroencephalogram-based Chinese Words Decoding</title>
      <link>https://example.com/publication/conference-paper/eegtcw-electroencephalogram-based-chinese-words-decoding/</link>
      <pubDate>Fri, 14 Apr 2023 15:57:35 +0000</pubDate>
      <guid>https://example.com/publication/conference-paper/eegtcw-electroencephalogram-based-chinese-words-decoding/</guid>
      <description></description>
    </item>
    
    <item>
      <title>MMES: Improved Mayfly Algorithm Based on Electrostatic Optimization Algorithm</title>
      <link>https://example.com/publication/conference-paper/mmes-improved-mayfly-algorithm-based-on-electrostatic-optimization-algorithm/</link>
      <pubDate>Fri, 09 Dec 2022 15:57:35 +0000</pubDate>
      <guid>https://example.com/publication/conference-paper/mmes-improved-mayfly-algorithm-based-on-electrostatic-optimization-algorithm/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Contact</title>
      <link>https://example.com/contact/</link>
      <pubDate>Mon, 24 Oct 2022 00:00:00 +0000</pubDate>
      <guid>https://example.com/contact/</guid>
      <description></description>
    </item>
    
    <item>
      <title>People</title>
      <link>https://example.com/people/</link>
      <pubDate>Mon, 24 Oct 2022 00:00:00 +0000</pubDate>
      <guid>https://example.com/people/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Tour</title>
      <link>https://example.com/tour/</link>
      <pubDate>Mon, 24 Oct 2022 00:00:00 +0000</pubDate>
      <guid>https://example.com/tour/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Feature-guided Multimodal Sentiment Analysis towards Industry 4.0</title>
      <link>https://example.com/publication/conference-paper/feature-guided-multimodal-sentiment-analysis-towards-industry-4.0/</link>
      <pubDate>Thu, 24 Mar 2022 15:57:35 +0000</pubDate>
      <guid>https://example.com/publication/conference-paper/feature-guided-multimodal-sentiment-analysis-towards-industry-4.0/</guid>
      <description></description>
    </item>
    
    <item>
      <title>IDCNN-CRF-based domain named entity recognition method</title>
      <link>https://example.com/publication/conference-paper/idcnn-crf-based-domain-named-entity-recognition-method/</link>
      <pubDate>Wed, 14 Oct 2020 15:57:35 +0000</pubDate>
      <guid>https://example.com/publication/conference-paper/idcnn-crf-based-domain-named-entity-recognition-method/</guid>
      <description></description>
    </item>
    
    <item>
      <title></title>
      <link>https://example.com/admin/config.yml</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://example.com/admin/config.yml</guid>
      <description></description>
    </item>
    
  </channel>
</rss>
