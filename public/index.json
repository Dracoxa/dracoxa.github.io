
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    [{"authors":["Qi Chen"],"categories":null,"content":"Qi Chen is a Master candidate with research interests in Multimodal Large Language Models (MLLM) and Agent systems. His work focuses on integrating multimodal data and intelligent agents.\nLorem ipsum dolor sit amet, consectetur adipiscing elit. Sed neque elit, tristique placerat feugiat ac, facilisis vitae arcu. Proin eget egestas augue. Praesent ut sem nec arcu pellentesque aliquet. Duis dapibus diam vel metus tempus vulputate.\n","date":1751328000,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":1751328000,"objectID":"926eee230b0a19babdc11ede8be8371a","permalink":"https://example.com/author/%E9%99%88%E5%A5%87/","publishdate":"2017-01-01T00:00:00Z","relpermalink":"/author/%E9%99%88%E5%A5%87/","section":"authors","summary":"Qi Chen is a Master candidate with research interests in Multimodal Large Language Models (MLLM) and Agent systems. His work focuses on integrating multimodal data and intelligent agents.\nLorem ipsum dolor sit amet, consectetur adipiscing elit. Sed neque elit, tristique placerat feugiat ac, facilisis vitae arcu. Proin eget egestas augue. Praesent ut sem nec arcu pellentesque aliquet. Duis dapibus diam vel metus tempus vulputate.\n","tags":null,"title":"陈奇","type":"authors"},{"authors":["Jingxuan Wei"],"categories":null,"content":"Jingxuan Wei is a PhD candidate with research interests in multimodal reasoning, machine translation, and AI for Science. His work focuses on advancing the integration of diverse data modalities and applying artificial intelligence to scientific discovery.\nLorem ipsum dolor sit amet, consectetur adipiscing elit. Sed neque elit, tristique placerat feugiat ac, facilisis vitae arcu. Proin eget egestas augue. Praesent ut sem nec arcu pellentesque aliquet. Duis dapibus diam vel metus tempus vulputate.\n","date":1751328000,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":1751328000,"objectID":"156552cc76ba020c53fdbabef12222b2","permalink":"https://example.com/author/%E9%AD%8F%E9%9D%96%E7%83%9C/","publishdate":"2025-06-23T00:00:00Z","relpermalink":"/author/%E9%AD%8F%E9%9D%96%E7%83%9C/","section":"authors","summary":"Jingxuan Wei is a PhD candidate with research interests in multimodal reasoning, machine translation, and AI for Science. His work focuses on advancing the integration of diverse data modalities and applying artificial intelligence to scientific discovery.\n","tags":null,"title":"魏靖烜","type":"authors"},{"authors":["Linzhuang Sun"],"categories":null,"content":"Linzhuang Sun is a PhD candidate with research interests in large language models and multimodal large language models. His work focuses on advancing the development and application of these advanced AI technologies.\nLorem ipsum dolor sit amet, consectetur adipiscing elit. Sed neque elit, tristique placerat feugiat ac, facilisis vitae arcu. Proin eget egestas augue. Praesent ut sem nec arcu pellentesque aliquet. Duis dapibus diam vel metus tempus vulputate.!\n","date":1750636800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":1750636800,"objectID":"669d651bd015741cfaac7fbbc2630ba4","permalink":"https://example.com/author/%E5%AD%99%E6%9E%97%E5%A3%AE/","publishdate":"2025-06-23T00:00:00Z","relpermalink":"/author/%E5%AD%99%E6%9E%97%E5%A3%AE/","section":"authors","summary":"Linzhuang Sun is a PhD candidate with research interests in large language models and multimodal large language models. His work focuses on advancing the development and application of these advanced AI technologies.\n","tags":null,"title":"孙林壮","type":"authors"},{"authors":["Gaowei Wu"],"categories":null,"content":"Gaowei Wu is a Master candidate with research interests in Agent and Retrieval-Augmented Generation (RAG). His work focuses on advancing intelligent agent systems and enhanced generation techniques.\nLorem ipsum dolor sit amet, consectetur adipiscing elit. Sed neque elit, tristique placerat feugiat ac, facilisis vitae arcu. Proin eget egestas augue. Praesent ut sem nec arcu pellentesque aliquet. Duis dapibus diam vel metus tempus vulputate.\n","date":1744646255,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":1744646255,"objectID":"4040019e893938f57df21441a40e78eb","permalink":"https://example.com/author/%E4%BC%8D%E9%AB%98%E5%B7%8D/","publishdate":"2023-04-14T15:57:35Z","relpermalink":"/author/%E4%BC%8D%E9%AB%98%E5%B7%8D/","section":"authors","summary":"Gaowei Wu is a Master candidate with research interests in Agent and Retrieval-Augmented Generation (RAG). His work focuses on advancing intelligent agent systems and enhanced generation techniques.\nLorem ipsum dolor sit amet, consectetur adipiscing elit. Sed neque elit, tristique placerat feugiat ac, facilisis vitae arcu. Proin eget egestas augue. Praesent ut sem nec arcu pellentesque aliquet. Duis dapibus diam vel metus tempus vulputate.\n","tags":null,"title":"伍高巍","type":"authors"},{"authors":["Caijun Jia"],"categories":null,"content":"贾彩军 is a master candidate with research interests in multimodal reasoning, machine translation, and AI for Science. His work focuses on advancing the integration of diverse data modalities and applying artificial intelligence to scientific discovery.\nLorem ipsum dolor sit amet, consectetur adipiscing elit. Sed neque elit, tristique placerat feugiat ac, facilisis vitae arcu. Proin eget egestas augue. Praesent ut sem nec arcu pellentesque aliquet. Duis dapibus diam vel metus tempus vulputate.\n","date":1744646255,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":1744646255,"objectID":"9820560ac29db6fbe3b9f4c136bd9248","permalink":"https://example.com/author/%E8%B4%BE%E5%BD%A9%E5%86%9B/","publishdate":"2023-04-14T15:57:35Z","relpermalink":"/author/%E8%B4%BE%E5%BD%A9%E5%86%9B/","section":"authors","summary":"贾彩军 is a master candidate with research interests in multimodal reasoning, machine translation, and AI for Science. His work focuses on advancing the integration of diverse data modalities and applying artificial intelligence to scientific discovery.\n","tags":null,"title":"贾彩军","type":"authors"},{"authors":["Bihui Yu"],"categories":null,"content":"Bihui Yu is a Researcher and Doctoral Supervisor at the Shenyang Institute of Computing Technology, Chinese Academy of Sciences, where he serves as the Head of Multimodal Cognitive Intelligence at the Technology Center and Deputy Director of the Liaoning Provincial Multimodal Intelligent Perception and Cognitive Engineering Research Center. His research focuses on big data, artificial intelligence, knowledge engineering (knowledge graphs), and multimodal data processing. He has led and participated in over 20 national, provincial, and municipal research projects, including the National Major Scientific Instrument Development Project and the National Key R\u0026amp;D Program. Bihui has published over 40 academic papers in domestic and international conferences and journals, applied for more than 10 national invention patents, and contributed to the development of products such as digital broadcasting terminals, communication scheduling terminals, industrial big data analysis platforms, and knowledge management platforms. He has extensive experience in big data, knowledge engineering, natural language processing, and network communication applications\n","date":1728230255,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":1728230255,"objectID":"cfb8515c3dfee36328e2155bdeea93ba","permalink":"https://example.com/author/%E4%BA%8E%E7%A2%A7%E8%BE%89/","publishdate":"2024-10-06T15:57:35Z","relpermalink":"/author/%E4%BA%8E%E7%A2%A7%E8%BE%89/","section":"authors","summary":"Bihui Yu is a Researcher and Doctoral Supervisor at the Shenyang Institute of Computing Technology, Chinese Academy of Sciences, where he serves as the Head of Multimodal Cognitive Intelligence at the Technology Center and Deputy Director of the Liaoning Provincial Multimodal Intelligent Perception and Cognitive Engineering Research Center. His research focuses on big data, artificial intelligence, knowledge engineering (knowledge graphs), and multimodal data processing. He has led and participated in over 20 national, provincial, and municipal research projects, including the National Major Scientific Instrument Development Project and the National Key R\u0026D Program. Bihui has published over 40 academic papers in domestic and international conferences and journals, applied for more than 10 national invention patents, and contributed to the development of products such as digital broadcasting terminals, communication scheduling terminals, industrial big data analysis platforms, and knowledge management platforms. He has extensive experience in big data, knowledge engineering, natural language processing, and network communication applications\n","tags":null,"title":"于碧辉","type":"authors"},{"authors":["Zhengbing Yao"],"categories":null,"content":"姚征兵 is a Master candidate with research interests in multimodal reasoning, machine translation, and AI for Science. His work focuses on advancing the integration of diverse data modalities and applying artificial intelligence to scientific discovery.\nLorem ipsum dolor sit amet, consectetur adipiscing elit. Sed neque elit, tristique placerat feugiat ac, facilisis vitae arcu. Proin eget egestas augue. Praesent ut sem nec arcu pellentesque aliquet. Duis dapibus diam vel metus tempus vulputate.\n","date":1728230255,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":1728230255,"objectID":"1e6937ff0affcdc255305abbb3874c4d","permalink":"https://example.com/author/%E5%A7%9A%E5%BE%81%E5%85%B5/","publishdate":"2024-10-06T15:57:35Z","relpermalink":"/author/%E5%A7%9A%E5%BE%81%E5%85%B5/","section":"authors","summary":"姚征兵 is a Master candidate with research interests in multimodal reasoning, machine translation, and AI for Science. His work focuses on advancing the integration of diverse data modalities and applying artificial intelligence to scientific discovery.\n","tags":null,"title":"姚征兵","type":"authors"},{"authors":["张思博"],"categories":null,"content":"张思博 is a Master candidate with research interests in multimodal reasoning, machine translation, and AI for Science. His work focuses on advancing the integration of diverse data modalities and applying artificial intelligence to scientific discovery.\nLorem ipsum dolor sit amet, consectetur adipiscing elit. Sed neque elit, tristique placerat feugiat ac, facilisis vitae arcu. Proin eget egestas augue. Praesent ut sem nec arcu pellentesque aliquet. Duis dapibus diam vel metus tempus vulputate.\n","date":1681487855,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":1681487855,"objectID":"82ecdf46e3323f53c4aecb3e3541218c","permalink":"https://example.com/author/%E5%BC%A0%E6%80%9D%E5%8D%9A/","publishdate":"2023-04-14T15:57:35Z","relpermalink":"/author/%E5%BC%A0%E6%80%9D%E5%8D%9A/","section":"authors","summary":"张思博 is a Master candidate with research interests in multimodal reasoning, machine translation, and AI for Science. His work focuses on advancing the integration of diverse data modalities and applying artificial intelligence to scientific discovery.\n","tags":null,"title":"张思博","type":"authors"},{"authors":["Dawei Liu"],"categories":null,"content":"Dawei Liu is a Master candidate with research interests iMultimodal Generation and Digital Human. His work focuses on integrating multimodal data and intelligent agents.\nLorem ipsum dolor sit amet, consectetur adipiscing elit. Sed neque elit, tristique placerat feugiat ac, facilisis vitae arcu. Proin eget egestas augue. Praesent ut sem nec arcu pellentesque aliquet. Duis dapibus diam vel metus tempus vulputate.\n","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"b670986c2f24df805705c1d21e713e45","permalink":"https://example.com/author/%E5%88%98%E5%A4%A7%E4%BC%9F/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/%E5%88%98%E5%A4%A7%E4%BC%9F/","section":"authors","summary":"Dawei Liu is a Master candidate with research interests iMultimodal Generation and Digital Human. His work focuses on integrating multimodal data and intelligent agents.\nLorem ipsum dolor sit amet, consectetur adipiscing elit. Sed neque elit, tristique placerat feugiat ac, facilisis vitae arcu. Proin eget egestas augue. Praesent ut sem nec arcu pellentesque aliquet. Duis dapibus diam vel metus tempus vulputate.\n","tags":null,"title":"刘大伟","type":"authors"},{"authors":["Mingfei Jin"],"categories":null,"content":"Mingfei Jin is a Master candidate with research interests in large language models, reinforcement learning, and reasoning chains. His work focuses on advancing AI reasoning and learning systems.\nLorem ipsum dolor sit amet, consectetur adipiscing elit. Sed neque elit, tristique placerat feugiat ac, facilisis vitae arcu. Proin eget egestas augue. Praesent ut sem nec arcu pellentesque aliquet. Duis dapibus diam vel metus tempus vulputate.\n","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"ca3ac3f5707f8bbc6275328e148d4189","permalink":"https://example.com/author/%E9%9D%B3%E6%98%8E%E9%A3%9E/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/%E9%9D%B3%E6%98%8E%E9%A3%9E/","section":"authors","summary":"Mingfei Jin is a Master candidate with research interests in large language models, reinforcement learning, and reasoning chains. His work focuses on advancing AI reasoning and learning systems.\nLorem ipsum dolor sit amet, consectetur adipiscing elit. Sed neque elit, tristique placerat feugiat ac, facilisis vitae arcu. Proin eget egestas augue. Praesent ut sem nec arcu pellentesque aliquet. Duis dapibus diam vel metus tempus vulputate.\n","tags":null,"title":"靳明飞","type":"authors"},{"authors":["Hexuan Jin"],"categories":null,"content":"Hexuan Jin is a Master candidate with research interests in Multimodal Large Language Models (MLLM) and Agent systems. His work focuses on integrating multimodal data and intelligent agents.\nLorem ipsum dolor sit amet, consectetur adipiscing elit. Sed neque elit, tristique placerat feugiat ac, facilisis vitae arcu. Proin eget egestas augue. Praesent ut sem nec arcu pellentesque aliquet. Duis dapibus diam vel metus tempus vulputate.\n","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"84f9f7b51ba4d9c1557ebdba69a155dc","permalink":"https://example.com/author/%E9%9D%B3%E8%B5%AB%E7%83%9C/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/%E9%9D%B3%E8%B5%AB%E7%83%9C/","section":"authors","summary":"Hexuan Jin is a Master candidate with research interests in Multimodal Large Language Models (MLLM) and Agent systems. His work focuses on integrating multimodal data and intelligent agents.\nLorem ipsum dolor sit amet, consectetur adipiscing elit. Sed neque elit, tristique placerat feugiat ac, facilisis vitae arcu. Proin eget egestas augue. Praesent ut sem nec arcu pellentesque aliquet. Duis dapibus diam vel metus tempus vulputate.\n","tags":null,"title":"靳赫烜","type":"authors"},{"authors":[],"categories":null,"content":"Slides can be added in a few ways:\nCreate slides using Wowchemy’s Slides feature and link using slides parameter in the front matter of the talk file Upload an existing slide deck to static/ and link using url_slides parameter in the front matter of the talk file Embed your slides (e.g. Google Slides) or presentation video on this page using shortcodes. Further event details, including page elements such as image galleries, can be added to the body of this page.\n","date":1906549200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1906549200,"objectID":"a8edef490afe42206247b6ac05657af0","permalink":"https://example.com/event/example/","publishdate":"2017-01-01T00:00:00Z","relpermalink":"/event/example/","section":"event","summary":"An example event.","tags":[],"title":"Example Event","type":"event"},{"authors":["魏靖烜","Cheng Tan","陈奇"],"categories":null,"content":"","date":1751328000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1751328000,"objectID":"67b1fdc1b98e518a9ee51d1b443d00f6","permalink":"https://example.com/publication/from-words-to-structured/","publishdate":"2017-01-01T00:00:00Z","relpermalink":"/publication/from-words-to-structured/","section":"publication","summary":"Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum.","tags":[],"title":"From Words to Structured Visuals: A Benchmark and Framework for Text-to-Diagram Generation and Editing","type":"publication"},{"authors":["Guiyong Chang","Liping Bu","魏靖烜","孙林壮","Dawei Liu"],"categories":null,"content":"","date":1750636800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1750636800,"objectID":"f5703b05bdee62f11e0a6755f60a60d4","permalink":"https://example.com/publication/multifaceted-assessment-and/","publishdate":"2025-06-23T00:00:00Z","relpermalink":"/publication/multifaceted-assessment-and/","section":"publication","summary":"With the evolution of artificial intelligence technology and the exploration of brain science, more and more researchers are committed to the decoding of brain signals, and the application of brain-computer interface(BCI) is constantly updated. Electroencephalogram(EEG) signals are non-invasive signals that have attracted increasing attention due to their non-transplantability and high temporal resolution. This paper focuses on the decoding of EEG signals to Chinese words, and proposes a novel framework, EEGTCW, which is based on the BiGRU-Attention-CNN classification model and data augmentation method to decode EEG signals into categories corresponding to Chinese words on a self-built dataset. The classification accuracy reaches 0.88, while exploring the gener-alization performance of the model under different batches of a single subject and generalization performance across subjects. It has certain reference value for the improvement of BCI assistant system for speech disorder people.","tags":[null],"title":"Multifaceted Assessment and Resolution of Hallucinations in Large Visual-Language Models","type":"publication"},{"authors":["魏靖烜","Nan Xu","Junnan Zhu","Yanni Hao","伍高巍","于碧辉","Lei Wang"],"categories":null,"content":"","date":1744646255,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1744646255,"objectID":"81530816a873a2849078a8cd0d1dd8ae","permalink":"https://example.com/publication/preprint/chartmind-a-comprehensive-benchmark-for-complex-real-world-multimodal-chart-question-answering/","publishdate":"2023-04-14T15:57:35Z","relpermalink":"/publication/preprint/chartmind-a-comprehensive-benchmark-for-complex-real-world-multimodal-chart-question-answering/","section":"publication","summary":"Chart question answering (CQA) has become a critical multimodal task for evaluating the reasoning capabilities of vision-language models. While early approaches have shown promising performance by focusing on visual features or leveraging large-scale pre-training, most existing evaluations rely on rigid output formats and objective metrics, thus ignoring the complex, real-world demands of practical chart analysis. In this paper, we introduce ChartMind, a new benchmark designed for complex CQA tasks in real-world settings. ChartMind covers seven task categories, incorporates multilingual contexts, supports open-domain textual outputs, and accommodates diverse chart formats, bridging the gap between real-world applications and traditional academic benchmarks. Furthermore, we propose a context-aware yet model-agnostic framework, ChartLLM, that focuses on extracting key contextual elements, reducing noise, and enhancing the reasoning accuracy of multimodal large language models. Extensive evaluations on ChartMind and three representative public benchmarks with 14 mainstream multimodal models show our framework significantly outperforms the previous three common CQA paradigms:instruction-following, OCR-enhanced, and chain-of-thought, highlighting the importance of flexible chart understanding for real-world CQA. These findings suggest new directions for developing more robust chart reasoning in future research.","tags":[],"title":"ChartReasoner: Code-Driven Modality Bridging for Long-Chain Reasoning in Chart Question Answering","type":"publication"},{"authors":["贾彩军","Nan Xu","魏靖烜","Qingli Wang,","Lei Wang, Bihui Yu,","Junnan Zhu"],"categories":null,"content":"","date":1744646255,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1744646255,"objectID":"a825e0e3e938142a0109c33ca1594832","permalink":"https://example.com/publication/preprint/chartreasoner-code-driven-modality-bridging-for-long-chain-reasoning-in-chart-question-answering/","publishdate":"2023-04-14T15:57:35Z","relpermalink":"/publication/preprint/chartreasoner-code-driven-modality-bridging-for-long-chain-reasoning-in-chart-question-answering/","section":"publication","summary":"Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum.","tags":[],"title":"ChartReasoner: Code-Driven Modality Bridging for Long-Chain Reasoning in Chart Question Answering","type":"publication"},{"authors":null,"categories":null,"content":"中国科学院沈阳计算所多模态认知智能团队“From Words to Structured Visuals: A Benchmark and Framework for Text-to-Diagram Generation and Editing”文章被计算机视觉与模式识别会议(CVPR 2025)接受，CVPR是计算机视觉领域最具权威、最高水平、最具影响力的国际顶级学术会议之一，为CCF A类会议。\nWe are excited to celebrate the acceptance of the paper “From Words to Structured Visuals: A Benchmark and Framework for Text-to-Diagram Generation and Editing” by Jingxuan Wei, Cheng Tan, Qi Chen, Gaowei Wu, Siyuan Li, Zhangyang Gao, Linzhuang Sun, Bihui Yu, and Ruifeng Guo at CVPR 2025!\nThis innovative work presents a pioneering benchmark and framework for text-to-diagram generation and editing. The paper introduces a comprehensive approach to transforming textual descriptions into structured visual diagrams, addressing challenges in automated diagram creation and iterative editing. By establishing a new benchmark, the authors provide a robust evaluation platform for future research in this interdisciplinary domain of computer vision and natural language processing. Their framework leverages advanced techniques to enable precise, context-aware diagram generation, offering significant potential for applications in education, technical documentation, and data visualization.\nThis achievement highlights the team’s dedication to advancing the integration of language and visual processing, paving the way for more intuitive and automated visualization tools. Congratulations to Jingxuan Wei, Cheng Tan, Qi Chen, Gaowei Wu, Siyuan Li, Zhangyang Gao, Linzhuang Sun, Bihui Yu, and Ruifeng Guo for their outstanding contribution to CVPR 2025!\nRead the full paper here: https://arxiv.org/pdf/2411.11916\n","date":1740614400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1740614400,"objectID":"26ba21cfc75e220d529131a7f9fa8ac4","permalink":"https://example.com/post/congratulations-on-cvpr-2025-publication/","publishdate":"2025-02-27T00:00:00Z","relpermalink":"/post/congratulations-on-cvpr-2025-publication/","section":"post","summary":"中国科学院沈阳计算所多模态认知智能团队“From Words to Structured Visuals: A Benchmark and Framework for Text-to-Diagram Generation and Editing”文章被计算机视觉与模式识别会议(CVPR 2025)接受，CVPR是计算机视觉领域最具权威、最高水平、最具影响力的国际顶级学术会议之一，为CCF A类会议。\n","tags":null,"title":"Congratulations on CVPR 2025 Publication","type":"post"},{"authors":["于碧辉","姚征兵","魏靖烜"],"categories":null,"content":"","date":1728230255,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1728230255,"objectID":"b2171c69f4b346c6d45a666eb7b655bb","permalink":"https://example.com/publication/faster-and-more-efficient-subject-image-generation-for-text-to-image-diffusion-models/","publishdate":"2024-10-06T15:57:35Z","relpermalink":"/publication/faster-and-more-efficient-subject-image-generation-for-text-to-image-diffusion-models/","section":"publication","summary":"Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum.","tags":[],"title":"Faster and More Efficient Subject Image Generation for Text-to-Image Diffusion Models","type":"publication"},{"authors":["Guiyong Chang,","Liping Bu,","魏靖烜","孙林壮","Dawei Liu"],"categories":null,"content":"","date":1728230255,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1728230255,"objectID":"1083139c7797649c7fd35844470c8771","permalink":"https://example.com/publication/sam-wav2lip++-enhancing-behavioral-realism-in-synthetic-agents-through-audio-driven-speech-and-action-refinement/","publishdate":"2024-10-06T15:57:35Z","relpermalink":"/publication/sam-wav2lip++-enhancing-behavioral-realism-in-synthetic-agents-through-audio-driven-speech-and-action-refinement/","section":"publication","summary":"Digital human generation is a forward-looking field in technology. Despite significant progress in the generation of speaking facial videos, many challenges remain unaddressed. Issues such as unnatural head movements, distorted expressions, artifacts in generated videos, and uncoordinated limb movements persist. Most current efforts are focused on specific individuals, with enhancements often limited to head movements without further advancing the overall behavioral actions of digital humans. In this context, we introduce a new dataset, CFMD, and a novel model, SAM-Wav2lip++, capable of generating consistent, audio-synchronized lip and behavior action videos from a single reference image of any identity. This work features three main innovative components:(1) a contrastive lip-sync discriminator for precise lip synchronization, (2) a generator for the synthesis of sound-action consistency, and (3) the SAM module for facial refinement operations. Through extensive experiments and user studies, our results demonstrate that our model can synthesize digital human videos of impressively high perceptual quality that accurately sync lip movements and behavioral actions with the input audio, substantially outperforming the state-of-the-art baselines evaluations.","tags":[],"title":"SAM-Wav2lip++: Enhancing Behavioral Realism in Synthetic Agents Through Audio-Driven Speech and Action Refinement","type":"publication"},{"authors":["魏靖烜","Cheng Tan","Zhangyang Gao"],"categories":null,"content":"","date":1724342255,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1724342255,"objectID":"d7e9ee5c02984ecdd2f90a84e2035ed6","permalink":"https://example.com/publication/interpretable-and-generalizable-spatiotemporal-predictive-learning-with-disentangled-consistency/","publishdate":"2024-08-22T15:57:35Z","relpermalink":"/publication/interpretable-and-generalizable-spatiotemporal-predictive-learning-with-disentangled-consistency/","section":"publication","summary":"Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum.","tags":[],"title":"Interpretable and Generalizable Spatiotemporal Predictive Learning with Disentangled Consistency","type":"publication"},{"authors":["Cheng Tan","魏靖烜","Zhangyang Gao"],"categories":null,"content":"","date":1719975407,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1719975407,"objectID":"ae53aedf050d11e3faeabfe7e7ce6334","permalink":"https://example.com/publication/boosting-the-power-of-small-multimodal-reasoning-models-to-match-larger-models-with-self-consistency-training/","publishdate":"2024-07-03T02:56:47Z","relpermalink":"/publication/boosting-the-power-of-small-multimodal-reasoning-models-to-match-larger-models-with-self-consistency-training/","section":"publication","summary":"Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum.","tags":[],"title":"Boosting the Power of Small Multimodal Reasoning Models to Match Larger Models with Self-Consistency Training","type":"publication"},{"authors":["Ruifeng Guo","魏靖烜","孙林壮"],"categories":null,"content":"","date":1717430255,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1717430255,"objectID":"d7c90ac6e03b5a8e16b82ea6c26ed8cb","permalink":"https://example.com/publication/a-survey-on-advancements-in-image-text-multimodal-models-from-general-techniques-to-biomedical-implementations/","publishdate":"2024-06-03T15:57:35Z","relpermalink":"/publication/a-survey-on-advancements-in-image-text-multimodal-models-from-general-techniques-to-biomedical-implementations/","section":"publication","summary":"Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum.","tags":[],"title":"A survey on advancements in image-text multimodal models: From general techniques to biomedical implementations","type":"publication"},{"authors":["魏靖烜","孙林壮","Yichong Leng"],"categories":null,"content":"","date":1713860996,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1713860996,"objectID":"b63ec37a763a92b947aab8cc946ce1c3","permalink":"https://example.com/publication/sentence-level-or-token-level-a-comprehensive-study-on-knowledge-distillation/","publishdate":"2024-04-23T08:29:56Z","relpermalink":"/publication/sentence-level-or-token-level-a-comprehensive-study-on-knowledge-distillation/","section":"publication","summary":"Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum.","tags":[],"title":"Sentence-Level or Token-Level: A Comprehensive Study on Knowledge Distillation","type":"publication"},{"authors":["于碧辉","Yiman Zhao","魏靖烜"],"categories":null,"content":"","date":1702051055,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1702051055,"objectID":"a43140d03fec38599b989aaa8e3a3be8","permalink":"https://example.com/publication/em-cnn-bilstm-causality-extraction-based-on-electra-and-dual-stage-attention-mechanism/","publishdate":"2023-12-08T15:57:35Z","relpermalink":"/publication/em-cnn-bilstm-causality-extraction-based-on-electra-and-dual-stage-attention-mechanism/","section":"publication","summary":"Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum.","tags":[],"title":"EM-CNN-BiLSTM: Causality extraction based on Electra and Dual-Stage Attention Mechanism","type":"publication"},{"authors":["于碧辉","孙林壮","魏靖烜"],"categories":null,"content":"","date":1699199855,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1699199855,"objectID":"c0cd999d92537e663d601acb9468ac23","permalink":"https://example.com/publication/ted-cs-textual-enhanced-sensitive-video-detection-with-common-sense-knowledge/","publishdate":"2023-11-05T15:57:35Z","relpermalink":"/publication/ted-cs-textual-enhanced-sensitive-video-detection-with-common-sense-knowledge/","section":"publication","summary":"Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum.","tags":[],"title":"TED-CS: Textual Enhanced Sensitive Video Detection with Common Sense Knowledge","type":"publication"},{"authors":["魏靖烜","Cheng Tan","Zhangyang Gao"],"categories":null,"content":"","date":1690300655,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1690300655,"objectID":"dae503edd8cfa8e01008a31b4578eb5e","permalink":"https://example.com/publication/enhancing-human-like-multi-modal-reasoning-a-new-challenging-dataset-and-comprehensive-framework/","publishdate":"2023-07-25T15:57:35Z","relpermalink":"/publication/enhancing-human-like-multi-modal-reasoning-a-new-challenging-dataset-and-comprehensive-framework/","section":"publication","summary":"Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum.","tags":[],"title":"Enhancing Human-like Multi-Modal Reasoning: A New Challenging Dataset and Comprehensive Framework","type":"publication"},{"authors":["于碧辉","魏靖烜","Shaojie He"],"categories":null,"content":"","date":1681487855,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1681487855,"objectID":"501c38b417722a3a6b30a06112096173","permalink":"https://example.com/publication/a-new-multimodal-video-detection-model-and-dataset/","publishdate":"2023-04-14T15:57:35Z","relpermalink":"/publication/a-new-multimodal-video-detection-model-and-dataset/","section":"publication","summary":"Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum.","tags":[],"title":"Construction of Sensitive Image Datasets Based on Generative Methods","type":"publication"},{"authors":["Chang Liu","Jie Zhang","于碧辉"],"categories":null,"content":"","date":1681487855,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1681487855,"objectID":"eca005f949f1afb855b8af3f0f44588d","permalink":"https://example.com/publication/construction-of-sensitive-image-datasets-based-on-generative-methods/","publishdate":"2023-04-14T15:57:35Z","relpermalink":"/publication/construction-of-sensitive-image-datasets-based-on-generative-methods/","section":"publication","summary":"Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum.","tags":[],"title":"Construction of Sensitive Image Datasets Based on Generative Methods","type":"publication"},{"authors":["张思博","Lili Zhou","ang Wang"],"categories":null,"content":"","date":1681487855,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1681487855,"objectID":"70ee4f429a05d4bb1c85d7122bf4174f","permalink":"https://example.com/publication/eegtcw-electroencephalogram-based-chinese-words-decoding/","publishdate":"2023-04-14T15:57:35Z","relpermalink":"/publication/eegtcw-electroencephalogram-based-chinese-words-decoding/","section":"publication","summary":"Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum.","tags":[],"title":"EEGTCW: Electroencephalogram-based Chinese Words Decoding","type":"publication"},{"authors":["Shaojie He","于碧辉","魏靖烜"],"categories":null,"content":"","date":1670601455,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1670601455,"objectID":"9e08d10f9d66adffc7ba55dfd8c1a522","permalink":"https://example.com/publication/mmes-improved-mayfly-algorithm-based-on-electrostatic-optimization-algorithm/","publishdate":"2022-12-09T15:57:35Z","relpermalink":"/publication/mmes-improved-mayfly-algorithm-based-on-electrostatic-optimization-algorithm/","section":"publication","summary":"Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum.","tags":[],"title":"MMES: Improved Mayfly Algorithm Based on Electrostatic Optimization Algorithm","type":"publication"},{"authors":null,"categories":null,"content":"","date":1666569600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1666569600,"objectID":"6d99026b9e19e4fa43d5aadf147c7176","permalink":"https://example.com/contact/","publishdate":"2022-10-24T00:00:00Z","relpermalink":"/contact/","section":"","summary":"","tags":null,"title":"Contact","type":"landing"},{"authors":null,"categories":null,"content":"","date":1666569600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1666569600,"objectID":"c1d17ff2b20dca0ad6653a3161942b64","permalink":"https://example.com/people/","publishdate":"2022-10-24T00:00:00Z","relpermalink":"/people/","section":"","summary":"","tags":null,"title":"People","type":"landing"},{"authors":null,"categories":null,"content":"","date":1666569600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1666569600,"objectID":"b0d61e5cbb7472bf320bf0ef2aaeb977","permalink":"https://example.com/tour/","publishdate":"2022-10-24T00:00:00Z","relpermalink":"/tour/","section":"","summary":"","tags":null,"title":"Tour","type":"landing"},{"authors":["于碧辉","魏靖烜","Bo Yu"],"categories":null,"content":"","date":1648137455,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1648137455,"objectID":"73e0248288bf552349300a5826304089","permalink":"https://example.com/publication/feature-guided-multimodal-sentiment-analysis-towards-industry-4.0/","publishdate":"2022-03-24T15:57:35Z","relpermalink":"/publication/feature-guided-multimodal-sentiment-analysis-towards-industry-4.0/","section":"publication","summary":"Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum.","tags":[],"title":"Feature-guided Multimodal Sentiment Analysis towards Industry 4.0","type":"publication"},{"authors":["于碧辉","魏靖烜"],"categories":null,"content":"","date":1602691055,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1602691055,"objectID":"2f4305bc12585da3f7d4ca95876c899d","permalink":"https://example.com/publication/idcnn-crf-based-domain-named-entity-recognition-method/","publishdate":"2020-10-14T15:57:35Z","relpermalink":"/publication/idcnn-crf-based-domain-named-entity-recognition-method/","section":"publication","summary":"Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum.","tags":[],"title":"IDCNN-CRF-based domain named entity recognition method","type":"publication"}]